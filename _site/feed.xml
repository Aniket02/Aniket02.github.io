<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-04-24T11:09:04-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Aniket Shah</title><subtitle>Data Science Portfolio</subtitle><author><name>Aniket Shah</name></author><entry><title type="html">Shakespeare plays analysis</title><link href="http://localhost:4000/Shakespeare/" rel="alternate" type="text/html" title="Shakespeare plays analysis" /><published>2019-04-22T00:00:00-04:00</published><updated>2019-04-22T00:00:00-04:00</updated><id>http://localhost:4000/Shakespeare</id><content type="html" xml:base="http://localhost:4000/Shakespeare/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The aim of this post will be to analyze Shakespeare plays and determine whether some of the plays or parts of plays were not written by Shakespeare. The code is detailed in the jupyter &lt;a href=&quot;/assets/static/notebooks/shakespeare/ShakespearePlayAttribution.ipynb&quot; target=&quot;_blank&quot;&gt;notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;A number of Shakespeare plays along with its sections are given in a data-set available in Kaggle. The data-set is cleaned by dropping the NA values. After that, various sub-section lines are merged into a single section. For example, the sub-sections like 1.1.1, 1.1.2, etc are merged into 1.1.&lt;/p&gt;

&lt;p&gt;Once the data is grouped into sections, various syntactic, semantic and functional features of the sections are extracted and clustered to identify the sections which have not been written by Shakespeare. Since, it is very unlikely that Shakespeare might not have any contribution to the whole play, our analysis was done on various sub-sections of the plays. Also, it will help us give
a more granular picture on the various sections across different plays which might not have been written by Shakespeare/&lt;/p&gt;

&lt;h2 id=&quot;feature-extraction&quot;&gt;Feature Extraction&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Word Analysis:&lt;/strong&gt; Stats such as average and standard deviation of the number of words in a sentence, and the diversity (number of unique words vs. total words) were collected for each section.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Punctuation analysis:&lt;/strong&gt; An author’s signature can include the unique combination of punctuation marks used in his/her writing. So, we takes stats of the number of commas, colons and semicolons user per sentence for each section.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BagOfWords:&lt;/strong&gt; This feature takes the top 100 words found in the entire text, and counts the occurence of each for every section. To not many columns (100), we reduce the dimensions by applying TruncatedSVD on the count matrix.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Funtional BagOfWords:&lt;/strong&gt; This feature is similar to the one above except that the vocabulary includes only functional words, modiﬁed for 16-17th century, that are known to help deﬁne an authors signature in word adjacency networks according to research. Functional words found in Shakespearan english which are different than modern english. Note, words like ’yt it’ represent it, ’wid wyth wytt wi wt wth’ represent with and are not misspellings. Functional word source: &lt;a href=&quot;http://web.mit.edu/segarra/www/papers/MSc_Thesis_Segarra.pdf&quot;&gt;MSc_Thesis_Segarra&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ScenesLSA:&lt;/strong&gt; This feature is the tf-idf matrix of all words in the collection that occur in atleast 75 sections, and reduced dimensionaly using TruncateSVD (LSA).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ngramTFIDF:&lt;/strong&gt; This feature is a tf-idf matrix of unigrams, bigrams and tri-grams that occur in atleast 75 sections, and no more than 20% of the sections. The average sentence length is approximately 3 accross the collection and hence this shoud capture some common sentenes/phrases across sections.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Couple of feaures like tf-idf matrix of functional words and parts-of-speech tags were
not found to be useful for the shakespeare corpus.&lt;/p&gt;

&lt;h2 id=&quot;clustering-and-ensemble-voting&quot;&gt;Clustering and Ensemble Voting&lt;/h2&gt;

&lt;p&gt;Once the different features are extracted, clustering is applied one each set of features to divide them into groups of two. One group of sections will correspond to Shakespeare and the other to non-Shakespeare authors. The majority group is given to Shakespeare, with the assumption that the majority of sections will have been written by him. After that, ensemble voting is done across the various clustering groups for different set of features. Then by using majority voting, we determine if a section is written by Shakespeare or not. For example, let’s say cluster 0 corresponds to Shakespeare plays and cluster 1 to non-Shakespeare plays. If section 1.1 belongs to cluster 0 for features 0, 1, 2 and 3, to cluster 1 for features 4 and 5, we can conclude that section 1.1 belongs to cluster 0 since it was found to be in cluster 0 for 4 different features. In case of ties, where the
section is present in equal number of cluster groups for various sections, we assign it to shakespeare, given the beneﬁt of doubt.&lt;/p&gt;

&lt;p&gt;After ensemble voting is done, various sections corresponding to non-Shakespeare authors are determined and printed for reference.&lt;/p&gt;

&lt;h3 id=&quot;different-types-of-clustering-and-the-optimum-number-of-clusters&quot;&gt;Different types of clustering and the optimum number of clusters&lt;/h3&gt;

&lt;p&gt;KMeans, DBScan and Agglomerative clustering (a type of hierarchical clustering) are tried. Silhouette score measure is used to identify the optimum number of clusters in each of the techniques. It was found that across the various features, using the Silhouette score, was found as the optimum cluster number for k-means and agglomerative clustering. It was also found that DBscan gave only one cluster for all the features, and hence, was dropped from our clustering methods.&lt;/p&gt;

&lt;h3 id=&quot;visualization-of-the-data-before-and-after-clustering-across-features&quot;&gt;Visualization of the data before and after clustering across features&lt;/h3&gt;

&lt;p&gt;Different types of visualization schemes like TSNE, PCA and LSA (with Truncated SVD) are used for visualizing the data before and after clustering. Pre-clustering visualization is done to check the distribution of the various sections for the different sets of features and to determine if the clustering done above can give good results. Post-clustering (k-means &amp;amp; Agglomerative)
visualization is done to validate if properly distinct clusters have been identified by clustering.&lt;/p&gt;

&lt;h4 id=&quot;lexical-features&quot;&gt;Lexical Features&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/shakespeare/LexicalFeatures.png&quot; alt=&quot;Lexical Word Features plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/shakespeare/LexicalFeaturesKMeans.png&quot; alt=&quot;Lexical Word Features plot with Kmeans clustering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/shakespeare/LexicalFeaturesAgglomerative.png&quot; alt=&quot;Lexical Word Features plot with Agglomerative clustering&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;punctual-features&quot;&gt;Punctual Features&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/shakespeare/PunctualFeatures.png&quot; alt=&quot;Punctual Word Features plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/shakespeare/PunctualFeaturesKMeans.png&quot; alt=&quot;Punctual Word Features plot with Kmeans clustering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/shakespeare/PunctualFeaturesAgglomerative.png&quot; alt=&quot;Punctual Word Features plot with Agglomerative clustering&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;A novel approach of feature extraction and ensemble voting by clustering is done here to determine the various play-sections which might have not been written by Shakespeare. An in-depth feature extraction is done from the plays to identify the textual features that alltogether, encompass the writing signature of Shakespeare. K-means an Agglomerative clustering methods are applied to create two clusters. The majority cluster written by Shakespeare and the second, minority cluster, belonging to other authors. It was found that around 100 sections out of total 732 sections had patterns different to those of Shakespearan written sections, and hence, possibly written by other authors.&lt;/p&gt;</content><author><name>Aniket Shah</name></author><category term="NLP" /><category term="Classification" /><summary type="html">Using NLP, determine whether some of the plays or parts of plays were not witten by Shakespeare</summary></entry><entry><title type="html">Summary of the Seminal paper on Dropouts</title><link href="http://localhost:4000/Dropouts/" rel="alternate" type="text/html" title="Summary of the Seminal paper on Dropouts" /><published>2019-04-22T00:00:00-04:00</published><updated>2019-04-22T00:00:00-04:00</updated><id>http://localhost:4000/Dropouts</id><content type="html" xml:base="http://localhost:4000/Dropouts/">&lt;p&gt;In this post, I summarize the paper titled, “&lt;strong&gt;Dropouts: A Simple Way to Prevent Neural Networks from
Overtting&lt;/strong&gt;” by Srivastava et al., publised in Journal of Machine Learning (2014).&lt;/p&gt;

&lt;p&gt;Dropouts is a regularization technique for neural networks that was introduced to reduce over-fitting introduced in models with millions of parameters, but trained on limited data. The best way to regularize a fixed-sized model is to average the predictions of all possible settings of the parameters, weighing each setting by it’s posterior probability given the training data, but this would involve unlimited computation and is prohibitively expensive. Even if computation is not an issue, large networks require large amounts of training data and there may not be enough data available to train different networks on different subsets of data.&lt;/p&gt;

&lt;p&gt;Dropouts addresses both these issues of computation and data availability. It provides a way of approximately combining exponentially many different neural networks efficiently. It does this by dropping out units at random, temporarily removing it from the network, along with all it’s incoming and outgoing connections. Training a neural network with dropout can be seen as training a collection of &lt;script type=&quot;math/tex&quot;&gt;2^n&lt;/script&gt; thinned networks with weight sharing, where each thinned network gets trained very rarely, if at all. How is this handled at test time, as it is not feasible to average the predictions from exponentially many thinned models?&lt;/p&gt;

&lt;p&gt;Instead, a simple approximate averaging method is used, where the weights of the network, with all it’s units at test time, are scaled-down versions of the trained weights. If a unit is retained with probability p during training, the outgoing weights of that unit are multiplied by p at test time. This ensures that for any hidden unit, the expected output (given dropout probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;) is the same as the actual output at test time.&lt;/p&gt;

&lt;p&gt;The authors applied dropout on multiple classification datasets in various domain, in vision (MNIST, SVHN, CIFAR-10/100, ImageNet), speech (TIMIT), text and genetics. In all, they looked at the state-of-the-art techniques with and without dropout, observing the test error. The improvement was much smaller in the text dataset compared to that for the vision and speech data sets. In all cases, the optimal dropout probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; was found to be 0.5 for hidden units and 0.8 for input units. Dropout breaks up co-adaptations and the activations of the hidden units become sparse, which is why it leads to lower generalization errors.&lt;/p&gt;</content><author><name>Aniket Shah</name></author><category term="Summary" /><category term="Dropouts" /><summary type="html">A summary of the seminal paper on dropouts by Srivastava et al.</summary></entry><entry><title type="html">Hospitilization Rates due to Opioid Abuse</title><link href="http://localhost:4000/Opioid_Crisis/" rel="alternate" type="text/html" title="Hospitilization Rates due to Opioid Abuse" /><published>2019-04-19T00:00:00-04:00</published><updated>2019-04-19T00:00:00-04:00</updated><id>http://localhost:4000/Opioid_Crisis</id><content type="html" xml:base="http://localhost:4000/Opioid_Crisis/">&lt;p&gt;This post is a summary of the analysis done by my teamates and I on hosptilization rates due to Opiod Abuse in the U.S and Canada. It was the final report for the Stat E-150 course at Harvard. You can also read the &lt;a href=&quot;/assets/static/Opioid_Abuse_Report.pdf&quot; target=&quot;_blank&quot;&gt;entire report&lt;/a&gt;. We also created a fun news bulletin video summarizing our findings.&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/wmePjBy_sZY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The opioid epidemic is at an all-time high. On average, 115 persons die from opioid abuse every day in the United States of America (U.S.) Between the years 1999 – 2016, the death toll from opioid overdose is estimated to be over 350,000 individuals (CDC, 2017).&lt;/p&gt;

&lt;p&gt;The U.S. is not the only country devastated by opioid overdose. Canada is also suffering staggeringly high rates of hospitalization and deaths attributed to opioid abuse. The death toll has more than tripled due to drug overdose in parts of Canada like British Columbia (B.C. Coroners Service, 2018).&lt;/p&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;h3 id=&quot;participants&quot;&gt;Participants&lt;/h3&gt;

&lt;p&gt;The data was comprised of over 100,000 male and female participants from both Canada and the U.S that were hospitalized due to opioid abuse. Participants were separated into three &lt;em&gt;age&lt;/em&gt; categories, &lt;em&gt;25 to 44, 45 to 64&lt;/em&gt; and &lt;em&gt;65 and above&lt;/em&gt;. This was a longitudinal observational study; therefore, patients were not recruited for the purpose of this analysis.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;The data set from the U.S. was extracted from the Healthcare Cost and Utilization Project (HCUP). The Canadian data set was provided by the Canadian Institute for Health Information (CIHI) which aggregated data regarding opioid trends from 2007-2015.&lt;/p&gt;

&lt;h3 id=&quot;procedure&quot;&gt;Procedure&lt;/h3&gt;

&lt;p&gt;All data sets had opioid rates categorized by only one predictor variable, however, we realized that to run any statistical models on a dataset with multiple predictors, the rates needed to be stratified. Since we evaluated age and gender, the data sets needed to have a rate for each combination of groups, for example, Male &amp;amp; Age 25-44, Female &amp;amp; Age 25-44, Male &amp;amp; Age 45-64, etc. Since the data sets only contained separate rates for each predictor variable, the data was stratified by taking the individual ratios of each group, and multiplying by the national rate.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Stratified Rate = \left(AnnualRate\right)\left(\frac{GenderSpecificRate}{TotalRateGender}\right)\left(\frac{AgeGroupSpecificRate}{TotalRateAge}\right)&lt;/script&gt;

&lt;p&gt;Our original plan was to run multiple regression for the combined data, however, once we started our analysis it became apparent that we needed to run ANOVA to specifically compare rates between the two countries. It was necessary to run an additional test because the rates in the U.S. were significantly higher than rates in Canada. Therefore, multiple regression models were built to identify patterns within each country separately, and a one-way ANOVA was run the for combined data. Details are provided in the report linked above.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Through our research we uncovered that in the U.S., the age group of &lt;em&gt;25 to 44 years&lt;/em&gt; is significant because it speaks to the pattern of drug abuse which mirrors the three waves of the rise in opioid-related deaths. The first wave was attributed to the 
use of prescription of opioids in the 1990’s as general pain management. During this time, opioids were being marketed as “safe” and “non-addictive.” Opioids acted as a gateway to stronger drugs like heroin, namely the second wave which began in 2010. It is believed that once students graduated college, they were able to support their drug habit. The third wave began in 2013 with higher rates of opioid related deaths associated with fentanyl (“Understanding the epidemic,” 2017).&lt;/p&gt;

&lt;p&gt;In comparison to the U.S., it was found in Canada that those aged &lt;em&gt;65 and older&lt;/em&gt; had a higher rate of hospitalization due to opioid abuse. It was noted that there was a difference in prescribing habits between Canada and the U.S. which resulted in an increase in hospitalization in the age category of &lt;em&gt;65 and older&lt;/em&gt;. We believe a contributing factor could be comorbidities in that age group due to drug interactions or general decline in health. Although the rate of drug abuse is higher in the U.S., it is important to note that hospitalizations in the &lt;em&gt;65 and older&lt;/em&gt; group are proportionally higher in Canada. In summation, our research draws important findings as it relates to the opioid epidemic and provides considerations for future studies.&lt;/p&gt;</content><author><name>Aniket Shah</name></author><category term="Opioid Abuse" /><category term="Regression" /><category term="SPSS" /><summary type="html">Opioid Crisis in US and Canada, Multiple Regression, IBM SPSS</summary></entry><entry><title type="html">Case Study in Yelp Reviews using Spark ML</title><link href="http://localhost:4000/Yelp_Reviews/" rel="alternate" type="text/html" title="Case Study in Yelp Reviews using Spark ML" /><published>2019-04-19T00:00:00-04:00</published><updated>2019-04-19T00:00:00-04:00</updated><id>http://localhost:4000/Yelp_Reviews</id><content type="html" xml:base="http://localhost:4000/Yelp_Reviews/">&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;p&gt;To predict the user id of a Yelp Review based on the content of the review. I have done this by building classification models based on past yelp data. Pyspark libraries with Spark 2.4 and Python 3.6.6 :: Anaconda, Inc. are used to cleanse and analyze the data. I executed the jupyter notebook on a CentoOS 7 VM with 16GB Ram, 8 processor cores.&lt;/p&gt;

&lt;h2 id=&quot;data-set&quot;&gt;Data Set:&lt;/h2&gt;

&lt;p&gt;The Yelp collection of reviews can be downloaded from the &lt;a href=&quot;https://www.kaggle.com/yelp-dataset/yelp-dataset#yelp_academic_dataset_review.json&quot; target=&quot;_blank&quot;&gt;Kaggle website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The data file of interest is:&lt;br /&gt;
File Name: yelp_academic_dataset_review.json&lt;br /&gt;
File Size: 4.39GB&lt;br /&gt;
File Format: JSON&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;data-exploration&quot;&gt;Data Exploration&lt;/h2&gt;

&lt;p&gt;The data contains 6 million rows with 9 columns. Star counts show that on average, the reviews are generally positive, with maximum of reviews being 5 stars.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/yelpReviews/StarRatings.jpg&quot; alt=&quot;Star Ratings&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plots below show that 90% of users have less than 5 reviews. There is a small peak at 30 reviews as it represents all users having 30 or more reviews.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/yelpReviews/userReviews.jpg&quot; alt=&quot;User Review Counts&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The jupyter notebook containing code for data exploration and visualization is &lt;a href=&quot;/assets/static/notebooks/yelpReviews/CaseStudyofYelpReviews_ShahAniket_DataVisualization.ipynb&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;model-analysis&quot;&gt;Model Analysis&lt;/h2&gt;

&lt;p&gt;I created logistic regression and naive bayes models with count features, TFIDF features, and TFIDF features combined with n-grams. The highest test set accuracy was 80% for logistic regression with uni-gram and bi-gram TFDIF matrix.&lt;/p&gt;

&lt;p&gt;The code below creates a TFIDF matrix of 1 word and 2 word combinations that occur in atleast 10 documents, to a maximum of 1000. Models are trained on the combined features, and test instance predictions are made. Jupyter notebook containing code for all models is &lt;a href=&quot;/assets/static/notebooks/yelpReviews/ClassificationModelsYelpReviews_AniketShah_SparkMLCode.ipynb&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;pipline-with-tfidf-and-n-grams&quot;&gt;Pipline with TFIDF and N-grams:&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;reviewsTF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashingTF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;words&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rawFeatures_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numFeatures&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reviewsIdf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rawFeatures_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minDocFreq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ngram&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NGram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;words&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ngrams_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reviewsTF2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashingTF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ngrams_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rawFeatures_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numFeatures&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reviewsIdf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rawFeatures_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minDocFreq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reviewsTF&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reviewsIdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reviewsTF2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reviewsIdf2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;finalPipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regexTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;VectorAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;features_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;features&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_userID&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipelineTFIDF2Fit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;finalPipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_user_reviews&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;top_user_reviews_TFIDF2_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipelineTFIDF2Fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_user_reviews&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_TFIDF2_Reviews&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_TFIDF2_Reviews&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_user_reviews_TFIDF2_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randomSplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12345&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/yelpReviews/logisticTFIDFNGRAM.jpg&quot; alt=&quot;Logistic with TFIDF and N-GRAMS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/yelpReviews/nbTFIDFNGRAM.jpg&quot; alt=&quot;Naive Bayes with TFIDF and N-GRAMS&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/yelpReviews/summaryAccuracy.jpg&quot; alt=&quot;Summary for all models&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was surprised that a simple model as logistic regression could give an accuracy of 80% for 76 classes, with just 600 reviews per review. The highest accuracy was achieved when bigrams were included which is expected as users would have unique ways of combining two words together. I did not remove stopwords in this case as how common words are used tell us more about the writing pattern of a user than specific topical words. My assumption is trigrams will improve the accuracy further or it could possibly overfit. Regardless, that would generate a very sparse matrix, which would need to be reduced using LDA for the algorithm to run faster.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;p&gt;Apache Spark ML &lt;a href=&quot;https://spark.apache.org/docs/latest/ml-guide.html&quot; target=&quot;_blank&quot;&gt;main guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35&quot; target=&quot;_blank&quot;&gt;Multi-class classification&lt;/a&gt; in Spark.&lt;/p&gt;</content><author><name>Aniket Shah</name></author><category term="Spark ML" /><category term="Regression" /><summary type="html">Predict the User id of a Yelp Review based on the content of the review</summary></entry></feed>